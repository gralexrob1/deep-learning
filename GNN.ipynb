{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "nlCNZsWlOSfP"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "buW3eZmrj1N2",
    "outputId": "825f2cc5-6c42-40b6-c811-de8e83815d1a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def format_pytorch_version(version):\n",
    "    return version.split(\"+\")[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "    return \"cu\" + version.replace(\".\", \"\")\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "# !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-geometric\n",
    "\n",
    "import torch_geometric.nn as graphnn\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\\nDevice: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "3UvCNG8FgdS-"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "We use the Protein-Protein Interaction (PPI) network dataset which includes:\n",
    "- 20 graphs for training \n",
    "- 2 graphs for validation\n",
    "- 2 graphs for testing\n",
    "\n",
    "One graph of the PPI dataset has on average 2372 nodes. Each node:\n",
    "- 50 features : positional gene sets / motif gene / immunological signatures ...\n",
    "- 121 (binary) labels : gene ontology sets (way to classify gene products like proteins).\n",
    "\n",
    "**This problem aims to predict, for a given PPI graph, the correct node's labels**.\n",
    "\n",
    "**It is a node (multi-level) classification task** (trained using supervised learning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwdNhvzVNkZB",
    "outputId": "64b6d3c5-43b2-420b-e5ce-0204fa1dd6be"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset = PPI(root=\"data/GNN/train\", split=\"train\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset = PPI(root=\"data/GNN/val\", split=\"val\")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = PPI(root=\"data/GNN/test\", split=\"test\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "n_features, n_classes = train_dataset[0].x.shape[1], train_dataset[0].y.shape[1]\n",
    "\n",
    "print(\"Number of samples in the train dataset: \", len(train_dataset))\n",
    "print(\"Number of samples in the val dataset: \", len(test_dataset))\n",
    "print(\"Number of samples in the test dataset: \", len(test_dataset))\n",
    "\n",
    "print(\"Number of features per node: \", n_features)\n",
    "print(\"Number of classes per node: \", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fcn, device, dataloader):\n",
    "    score_list_batch = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        output = model(batch.x, batch.edge_index)\n",
    "        loss_test = loss_fcn(output, batch.y)\n",
    "        predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
    "        score = f1_score(batch.y.cpu().numpy(), predict, average=\"micro\")\n",
    "        score_list_batch.append(score)\n",
    "\n",
    "    return np.array(score_list_batch).mean()\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader\n",
    "):\n",
    "\n",
    "    epoch_list = []\n",
    "    scores_list = []\n",
    "\n",
    "    # loop over epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        # loop over batches\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_device = train_batch.to(device)\n",
    "            # logits is the output of the model\n",
    "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
    "            # compute the loss\n",
    "            loss = loss_fcn(logits, train_batch_device.y)\n",
    "            # optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        loss_data = np.array(losses).mean()\n",
    "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            # evaluate the model on the validation set\n",
    "            # computes the f1-score (see next function)\n",
    "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
    "            print(\"F1-Score: {:.4f}\".format(score))\n",
    "            scores_list.append(score)\n",
    "            epoch_list.append(epoch)\n",
    "\n",
    "    return epoch_list, scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_score(epoch_list, scores):\n",
    "    plt.figure(figsize=[10, 5])\n",
    "    plt.plot(epoch_list, scores)\n",
    "    plt.title(\"Evolution of F1S-Score w.r.t epochs\")\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "hiCcn9qeO6Nm"
   },
   "source": [
    "### Graph Convolution\n",
    "\n",
    "https://arxiv.org/pdf/1609.02907.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "2Km-GN1aMpd_"
   },
   "outputs": [],
   "source": [
    "class ConvGraphModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.graphconv1 = graphnn.GCNConv(input_size, hidden_size)\n",
    "        self.graphconv2 = graphnn.GCNConv(hidden_size, hidden_size)\n",
    "        self.graphconv3 = graphnn.GCNConv(hidden_size, output_size)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.graphconv1(x, edge_index)\n",
    "        x = self.elu(x)\n",
    "        x = self.graphconv2(x, edge_index)\n",
    "        x = self.elu(x)\n",
    "        x = self.graphconv3(x, edge_index)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "convolution_model = ConvGraphModel(\n",
    "    input_size=n_features, hidden_size=256, output_size=n_classes\n",
    ").to(device)\n",
    "\n",
    "convolution_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xkqo7e0gNACE",
    "outputId": "5f2ab59d-7841-4d79-da1b-2d7b4ed963c2"
   },
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "\n",
    "loss_fcn = nn.BCEWithLogitsLoss() # sigmoid included \n",
    "optimizer = torch.optim.Adam(convolution_model.parameters(), lr=0.005)\n",
    "\n",
    "epoch_list, convolution_model_scores = train(\n",
    "    convolution_model,\n",
    "    loss_fcn,\n",
    "    device,\n",
    "    optimizer,\n",
    "    max_epochs,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "ztfbbg2TNP7F",
    "outputId": "06c3c0c7-2ceb-4cee-a4d7-a654fa127a57"
   },
   "outputs": [],
   "source": [
    "score_test = evaluate(convolution_model, loss_fcn, device, test_dataloader)\n",
    "print(\"Convolution Model : F1-Score on the test set: {:.4f}\".format(score_test))\n",
    "\n",
    "plot_f1_score(epoch_list, convolution_model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "TRVsy5vLnWm_"
   },
   "source": [
    "### Graph Attention\n",
    "\n",
    "https://arxiv.org/pdf/1710.10903.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "qTo7PxFpRHzL"
   },
   "outputs": [],
   "source": [
    "class AttGraphModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"Default initialization is Glorot\"\"\"\n",
    "\n",
    "        self.graphat1 = graphnn.GATConv(input_size, hidden_size, heads=4)\n",
    "        self.graphat2 = graphnn.GATConv(4*hidden_size, hidden_size, heads=4)\n",
    "\n",
    "        \"\"\"Last layer averages features\"\"\"\n",
    "\n",
    "        self.graphat3 = graphnn.GATConv(4*hidden_size, output_size, heads=6, concat=False)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        \"\"\"Forward with skipped connections\"\"\"\n",
    "\n",
    "        x = self.graphat1(x, edge_index)\n",
    "        x1 = self.elu(x)\n",
    "\n",
    "        x = self.graphat2(x1, edge_index) \n",
    "        x2 = self.elu(x+x1)\n",
    "        \n",
    "        x = self.graphat3(x2, edge_index)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "attention_model = AttGraphModel(\n",
    "    input_size=n_features, hidden_size=256, output_size=n_classes\n",
    ").to(device)\n",
    "\n",
    "attention_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "6XIYzkYRo3AQ"
   },
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "\n",
    "loss_fcn = nn.BCEWithLogitsLoss() # sigmoid included \n",
    "optimizer = torch.optim.Adam(attention_model.parameters(), lr=0.005)\n",
    "\n",
    "epoch_list, attention_model_scores = train(attention_model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test = evaluate(convolution_model, loss_fcn, device, test_dataloader)\n",
    "print(\"Attention Model : F1-Score on the test set: {:.4f}\".format(score_test))\n",
    "\n",
    "plot_f1_score(epoch_list, convolution_model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "aWatNTPBpQGY"
   },
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_scores(epoch_list, convolution_model_scores, attention_model_scores):\n",
    "    plt.figure(figsize=[10, 5])\n",
    "    plt.plot(epoch_list, convolution_model_scores, \"b\", label=\"Basic Model\")\n",
    "    plt.plot(epoch_list, attention_model_scores, \"r\", label=\"Student Model\")\n",
    "    plt.title(\"Evolution of f1 score w.r.t epochs\")\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.ylabel(\"Epochs\")\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "WFWMqwDuSj7b"
   },
   "outputs": [],
   "source": [
    "score_test = evaluate(attention_model, loss_fcn, device, test_dataloader)\n",
    "print(\"Student Model : F1-Score on the test set: {:.4f}\".format(score_test))\n",
    "\n",
    "plot_f1_scores(epoch_list, convolution_model_scores, attention_model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "78jnqb8nWFJU",
    "id": "i9aVEYkuR3fp"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We tried to reproduce the Graph Attention Networks paper. The attention mechanism gives an advantage compared to GCN because it allows to attributes different weights to the different neighbors of a node rather than doing a simple summation and therefore have more granularity on the interesting features. GATConv can choose to emphasize or de-emphasize features from neighbors whereas GCNConv treats all neighbors equally.   \n",
    "Furthermore GATConv is able to capture long-range dependencies with the attention mechanism, contrary to GCNConv that only focuses on immediate neighbors.\n",
    "Finally, it can be worth noting that GAT can be parallelized when multiple heads are used, leading to even more efficiency without big time addition.  \n",
    "\n",
    "As guessed above, the result are better in term of F1 score with the GAT layers.  \n",
    "SOA is at 0.973 which is not so far away from our result.\n",
    "\n",
    "\n",
    "**The oversmoothing problem**  \n",
    "\n",
    "Oversmoothing in GNN corresponds to the phenomenon where the increase of the network depth leads to homogeneous nodes representations. The nodes features tend to become more similar.  \n",
    "In *Rush, Bronstein, Michra, A survey of oversmoothing in GNN, 2023* it is defined as \"the exponential\n",
    "convergence of all node features towards the same constant value as the number of layers in the GNN increases\".  \n",
    "The oversmoothing phenomenon is an issue because as nodes become more and more similar, we lose the ability to capture the differences between them and therefore lose performance.\n",
    "\n",
    "As per the same article, there are solutions to mitigate over-smoothing like normalization and regularization, or residual connections.    \n",
    "\n",
    "In our code there is no no regularization nor dropout. However by essence the attention mechanism adress the oversmoothing issue by capturing long-range depencies with different importance weights per nodes and per heads. Furthermore, skipping connection can help mitigate over-smoothing.  \n",
    "Based on these observations, we can say that the model is rather robust to oversmoothing."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "78jnqb8nWFJU",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
